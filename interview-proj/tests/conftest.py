"""Test configuration and fixtures"""
import pytest
import tempfile
import shutil
import os
from unittest.mock import Mock, patch

from src.model.config import ModelConfig
from src.model.generator import JenosizeTrendGenerator


@pytest.fixture(scope="session")
def test_config():
    """Test configuration fixture"""
    config = ModelConfig()
    config.max_length = 256  # Smaller for faster testing
    config.temperature = 0.7
    return config


@pytest.fixture
def mock_generator():
    """Mock generator fixture"""
    generator = Mock(spec=JenosizeTrendGenerator)
    generator.generate_article.return_value = {
        "title": "Test Article: Mock Generation",
        "content": "This is a test article generated by the mock generator. " * 20,
        "metadata": {
            "category": "Technology",
            "keywords": ["test", "mock"],
            "target_audience": "Developers",
            "tone": "Professional",
            "word_count": 240,
            "model": "mock_generator",
            "generation_type": "mock",
            "generated_at": "2024-01-01T00:00:00",
            "generation_time_seconds": 0.1
        }
    }
    generator.get_model_info.return_value = {
        "ai_available": False,
        "model_loaded": False,
        "cache_enabled": True,
        "generation_count": 0
    }
    return generator


@pytest.fixture
def temp_cache_dir():
    """Temporary cache directory fixture"""
    temp_dir = tempfile.mkdtemp(prefix="test_cache_")
    yield temp_dir
    shutil.rmtree(temp_dir, ignore_errors=True)


@pytest.fixture
def sample_article_data():
    """Sample article data for testing"""
    return {
        "topic": "AI in Software Development",
        "category": "Technology",
        "keywords": ["artificial intelligence", "software", "development", "automation"],
        "target_audience": "Software Developers",
        "tone": "Technical and Detailed"
    }


@pytest.fixture(autouse=True)
def setup_test_environment():
    """Setup test environment before each test"""
    # Set test environment variables
    os.environ["TESTING"] = "true"
    
    # Patch logging to reduce noise during tests
    with patch('src.model.generator.logger') as mock_logger:
        mock_logger.info = Mock()
        mock_logger.warning = Mock() 
        mock_logger.error = Mock()
        yield
    
    # Cleanup
    os.environ.pop("TESTING", None)


@pytest.fixture
def mock_torch():
    """Mock torch for testing without GPU dependencies"""
    with patch('src.model.generator.torch') as mock_torch:
        mock_torch.cuda.is_available.return_value = False
        mock_torch.device.return_value = Mock()
        mock_torch.cuda.empty_cache = Mock()
        mock_torch.cuda.memory_allocated.return_value = 0
        mock_torch.cuda.memory_reserved.return_value = 0
        yield mock_torch


@pytest.fixture
def mock_transformers():
    """Mock transformers for testing without model dependencies"""
    with patch('src.model.generator.AutoTokenizer') as mock_tokenizer, \
         patch('src.model.generator.AutoModelForCausalLM') as mock_model, \
         patch('src.model.generator.GenerationConfig') as mock_config:
        
        # Setup tokenizer mock
        tokenizer_instance = Mock()
        tokenizer_instance.pad_token = None
        tokenizer_instance.eos_token = "<eos>"
        tokenizer_instance.pad_token_id = 0
        tokenizer_instance.eos_token_id = 1
        mock_tokenizer.from_pretrained.return_value = tokenizer_instance
        
        # Setup model mock
        model_instance = Mock()
        model_instance.eval = Mock()
        model_instance.to.return_value = model_instance
        model_instance.generate.return_value = [Mock()]
        mock_model.from_pretrained.return_value = model_instance
        
        # Setup generation config mock
        config_instance = Mock()
        mock_config.return_value = config_instance
        
        yield {
            'tokenizer': mock_tokenizer,
            'model': mock_model,
            'config': mock_config,
            'tokenizer_instance': tokenizer_instance,
            'model_instance': model_instance
        }


@pytest.fixture
def performance_metrics():
    """Performance metrics tracking fixture"""
    import time
    
    metrics = {
        "start_time": time.time(),
        "memory_usage": [],
        "response_times": []
    }
    
    yield metrics
    
    # Calculate final metrics
    metrics["total_time"] = time.time() - metrics["start_time"]
    if metrics["response_times"]:
        metrics["avg_response_time"] = sum(metrics["response_times"]) / len(metrics["response_times"])
        metrics["max_response_time"] = max(metrics["response_times"])


class TestDataBuilder:
    """Helper class for building test data"""
    
    @staticmethod
    def create_article_request(**kwargs):
        """Create article request with defaults"""
        defaults = {
            "topic": "Test Topic",
            "category": "Technology",
            "keywords": ["test", "example"],
            "target_audience": "Testers",
            "tone": "Professional"
        }
        defaults.update(kwargs)
        return defaults
    
    @staticmethod
    def create_article_response(**kwargs):
        """Create article response with defaults"""
        defaults = {
            "success": True,
            "title": "Test Article Title",
            "content": "Test article content " * 50,
            "metadata": {
                "category": "Technology",
                "keywords": ["test", "example"],
                "target_audience": "Testers",
                "tone": "Professional",
                "word_count": 250,
                "model": "test_model",
                "generation_type": "test",
                "generated_at": "2024-01-01T00:00:00"
            },
            "message": "Article generated successfully"
        }
        defaults.update(kwargs)
        return defaults


@pytest.fixture
def test_data_builder():
    """Test data builder fixture"""
    return TestDataBuilder()


# Pytest configuration
def pytest_configure(config):
    """Configure pytest"""
    # Add custom markers
    config.addinivalue_line(
        "markers", "slow: marks tests as slow (deselect with '-m \"not slow\"')"
    )
    config.addinivalue_line(
        "markers", "integration: marks tests as integration tests"
    )
    config.addinivalue_line(
        "markers", "performance: marks tests as performance tests"
    )


def pytest_collection_modifyitems(config, items):
    """Modify collected test items"""
    # Automatically mark slow tests
    for item in items:
        if "performance" in item.nodeid or "concurrent" in item.nodeid:
            item.add_marker(pytest.mark.slow)
        
        if "integration" in item.nodeid or "end_to_end" in item.nodeid:
            item.add_marker(pytest.mark.integration)